# -*- coding: utf-8 -*-
"""practice_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_hCE_MvEZyS9KcECAQ35aZ-xT6wZerx
"""

import numpy as np
import pandas as pd
import seaborn as sns

"""# MITBIH Dataset"""

train_df_mitbih = pd.read_csv('/content/drive/MyDrive/Dataset for biomedical signals/mitbih_train.csv', header=None)
train_df_mitbih

equilibre=train_df_mitbih[187].value_counts()
equilibre

# Plot the frequency of each class
sns.set(style="whitegrid")
ax = sns.barplot(x=equilibre.index, y=equilibre)
ax.set(xlabel='Classes', ylabel='Frequency')
ax.set_title('Frequency of each class in the dataset')
ax

from google.colab import drive
drive.mount('/content/drive')

# Plot a random sample of each class
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
for i in range(5):
    plt.subplot(2,3,i+1)
    plt.plot(train_df_mitbih[train_df_mitbih[187]==i].iloc[0,:186])
    plt.title('Record for Class '+str(i))
# Big title for all
plt.suptitle('Random ECG for each class', size=16)
plt.show()

# Plot all data of each class
plt.figure(figsize=(20,10))
for i in range(5):
    plt.subplot(2,3,i+1)
    plt.plot(train_df_mitbih[train_df_mitbih[187]==i].iloc[:,0:186].T)
    plt.title('Record for Class '+str(i))
# Big title for all
plt.suptitle('All ECG for each class', size=16)
plt.show()

# Total number of rows
total_rows = train_df_mitbih.shape[0]
total_rows/5

# Balance the dataset by sampling each class to have 17500 samples
df_n = train_df_mitbih[train_df_mitbih[187]==0].sample(n=17500, random_state=42)
# Upsample the other classes to have 17500 samples
df_q = train_df_mitbih[train_df_mitbih[187]==1].sample(n=17500, random_state=42, replace=True)
df_v = train_df_mitbih[train_df_mitbih[187]==2].sample(n=17500, random_state=42, replace=True)
df_s = train_df_mitbih[train_df_mitbih[187]==3].sample(n=17500, random_state=42, replace=True)
df_f = train_df_mitbih[train_df_mitbih[187]==4].sample(n=17500, random_state=42, replace=True)

# Train and test data after balancing
train_df_mitbih_balanced = pd.concat([df_n, df_q, df_v, df_s, df_f])
train_df_mitbih_balanced

# Plot the frequency of each class after balancing
equilibre=train_df_mitbih_balanced[187].value_counts()
sns.set(style="whitegrid")
ax = sns.barplot(x=equilibre.index, y=equilibre)
ax.set(xlabel='Classes', ylabel='Frequency')
ax.set_title('Frequency of each class in the dataset after balancing')
ax

# Divide Valid and test data from the 'mitbih_test.csv' file (balanced)
test_df_mitbih = pd.read_csv('/content/drive/MyDrive/Dataset for biomedical signals/mitbih_test.csv', header=None)
valid_df_mitbih = test_df_mitbih.sample(frac=0.5, random_state=42)
test_df_mitbih = test_df_mitbih.drop(valid_df_mitbih.index)
test_df_mitbih

# Count the number of each class in valid and test data
print(valid_df_mitbih[187].value_counts())
print(test_df_mitbih[187].value_counts())

# Plot frequency of each class in valid and test data in 2 subplots
plt.figure(figsize=(20,10))
plt.subplot(1,2,1)
sns.set(style="whitegrid")
ax = sns.barplot(x=valid_df_mitbih[187].value_counts().index, y=valid_df_mitbih[187].value_counts())
ax.set(xlabel='Classes', ylabel='Frequency')
ax.set_title('Frequency of each class in the valid dataset')
plt.subplot(1,2,2)
sns.set(style="whitegrid")
ax = sns.barplot(x=test_df_mitbih[187].value_counts().index, y=test_df_mitbih[187].value_counts())
ax.set(xlabel='Classes', ylabel='Frequency')
ax.set_title('Frequency of each class in the test dataset')
plt.show()

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary


# Create a deep convolutional neural network model with residual connections
# Input: 187 features
# Output: 5 classes
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, 5)
        self.conv2 = nn.Conv1d(32, 64, 5)
        self.conv3 = nn.Conv1d(64, 128, 5)
        self.pool = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(128*19, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 5)


    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = nn.Flatten()(x)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Summary of the model
model = Net()
summary(model, (1, 187))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

# Convert the dataset to PyTorch tensors
class ECGDataset(torch.utils.data.Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

X_train = train_df_mitbih_balanced.iloc[:, :-1].values
y_train = train_df_mitbih_balanced.iloc[:, -1].values
X_valid = valid_df_mitbih.iloc[:, :-1].values
y_valid = valid_df_mitbih.iloc[:, -1].values
X_test = test_df_mitbih.iloc[:, :-1].values
y_test = test_df_mitbih.iloc[:, -1].values

train_dataset = ECGDataset(X_train, y_train)
valid_dataset = ECGDataset(X_valid, y_valid)
test_dataset = ECGDataset(X_test, y_test)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Net().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

from tqdm.auto import tqdm

# Train the model with validation
n_epochs = 20
train_losses = []
train_accuracies = []
valid_losses = []
valid_accuracies = []
for epoch in range(n_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs}', leave=False)):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)
    train_losses.append(running_loss/len(train_loader))
    train_accuracies.append(correct/total)

    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for i, (inputs, labels) in enumerate(valid_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    valid_losses.append(running_loss/len(valid_loader))
    valid_accuracies.append(correct/total)

    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracies[-1]:.4f}')

# Plot the training and validation loss
plt.figure(figsize=(10,5))
plt.plot(train_losses, label='Train Loss')
plt.plot(valid_losses, label='Valid Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy
plt.figure(figsize=(10,5))
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(valid_accuracies, label='Valid Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Test the model in the test dataset
model.eval()
test_acc = 0

# Classification report
y_pred = []
y_true = []
for X, y in test_loader:
    X, y = X.to(device), y.to(device)
    output = model(X)
    y_pred.extend(output.argmax(1).tolist())
    y_true.extend(y.tolist())
    test_acc += (output.argmax(1) == y).sum().item()
print(classification_report(y_true, y_pred))
print(f"Accuracy Score: {test_acc/len(test_dataset)}")

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
plt.figure(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt=".2f", cmap='Blues')
plt.title('Confusion Matrix')
plt.show()